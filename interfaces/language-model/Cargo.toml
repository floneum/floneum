[package]
name = "kalosm-language-model"
version = "0.3.0"
edition = "2021"
description = "A common interface for language models/transformers "
license = "MIT/Apache-2.0"
repository = "https://github.com/floneum/floneum"
authors = ["Evan Almloff"]
keywords = ["ai", "llm", "llama", "mistral", "nlp"]

[dependencies]
futures-util = "0.3.28"
llm-samplers = { workspace = true }
log = "0.4.17"
rand = "0.8.5"
tokio = { version = "1.28.1", features = ["sync"] }
serde = { version = "1.0.163", features = ["derive"], optional = true }
once_cell = "1.18.0"
anyhow = "1.0.71"
tracing = "0.1.37"
async-openai = { version = "0.14.2", optional = true }
async-trait = "0.1.73"
candle-core.workspace = true
kalosm-sample = { workspace = true }
kalosm-common.workspace = true
kalosm-streams.workspace = true
rayon = "1.10.0"
postcard = { version = "1.0.8", features = ["use-std"], optional = true }
thiserror = "1.0.61"
lru = { version = "0.12.3", optional = true }
safetensors = { version = "0.4.3", optional = true }
tokenizers = { workspace = true }

[dev-dependencies]
tokio = { version = "1.28.1", features = ["full"] }
kalosm = { workspace = true, features = ["language"] }
kalosm-learning = { workspace = true }

[features]
default = ["cache"]
remote = ["async-openai"]
serde = ["dep:serde", "safetensors"]
cache = ["serde", "dep:postcard", "dep:lru"]
