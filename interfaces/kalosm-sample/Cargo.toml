[package]
name = "kalosm-sample"
version = "0.2.0"
edition = "2021"
description = "A common interface for token sampling and helpers for structered llm sampling "
license = "MIT/Apache-2.0"
repository = "https://github.com/floneum/floneum"
authors = ["Evan Almloff"]
keywords = ["ai", "llm", "llama", "mistral", "nlp"]

[dependencies]
llm-samplers = { workspace = true }
rand = "0.8.5"
anyhow = "1.0.71"
tracing = "0.1.37"
llm = { git = "https://github.com/rustformers/llm", optional = true }
candle-core.workspace = true
tokenizers = { version = "0.13.4" }
rustc-hash = "1.1.0"
regex-automata = "0.4.5"

[features]
llamacpp = ["llm"]
