use fusor_gguf::GgmlType;

use crate::{
    Device,
    mir::{
        inputs::{QMatrixInput, TensorInput},
        kernel::GenericKernel,
        workgroup_shape::WorkgroupShape,
    },
    quantized::matmul::{
        QMatMulOperation,
        sgemv::{
            general::general_sgemv, q_8_0::q_8_0_sgemv, q_n::q_n_sgemv, q4k::q4k_sgemv,
            q6k::q6k_sgemv,
        },
    },
};
use crate::{
    QMatrix,
    quantized::matmul::sgemv::{
        q_8_0::Q_8_0_SGEMV_CHUNK_SIZE, q_n::Q_N_SGEMV_CHUNK_SIZE, q4k::Q4K_SGEMV_CHUNK_SIZE,
        q6k::Q6K_SGEMV_CHUNK_SIZE,
    },
};

mod general;
pub mod q4k;
pub mod q6k;
pub mod q_8_0;
pub mod q_n;

pub(crate) const SGEMV_CHUNK_SIZE: u32 = 2; // This is the size of the chunk each thread will process at a time
pub(crate) const SGEMV_VECTOR_SIZE: u32 = 4; // This is the size of the chunk we will dot at a time

/// Check if the device can support specialized SGEMV kernels that require 2 subgroups per workgroup.
/// This requires the workgroup to be large enough to fit 2 subgroups, which means:
/// max_subgroup_size >= 2 * min_subgroup_size
fn can_use_specialized_sgemv(device: &Device) -> bool {
    if !device.subgroups_supported() {
        return false;
    }
    // The workgroup is constrained to be <= max_subgroup_size, so we need
    // max_subgroup_size >= 2 * min_subgroup_size to fit 2 subgroups
    device.max_subgroup_size() >= 2 * device.min_subgroup_size()
}

#[allow(clippy::too_many_arguments)]
pub(crate) fn sgemv(
    op: &QMatMulOperation,
    generic_kernel: &mut GenericKernel,
    workgroup_size: &WorkgroupShape,
    input_a: &TensorInput,
    input_b: &QMatrixInput,
    output: &TensorInput,
    _n_size: &str,
    _m_size: &str,
    k_size: &str,
    graph: &crate::compute_graph::ComputeGraphInner,
) {
    let device = &graph.device;
    // Check if we can use specialized SGEMV (requires 2 subgroups per workgroup)
    let use_specialized = can_use_specialized_sgemv(device);
    match op.matrix.datatype {
        GgmlType::Q6K if use_specialized => q6k_sgemv(
            op,
            generic_kernel,
            workgroup_size,
            input_a,
            input_b,
            output,
            _n_size,
            _m_size,
            k_size,
        ),
        GgmlType::Q4K if use_specialized => q4k_sgemv(
            op,
            generic_kernel,
            workgroup_size,
            input_a,
            input_b,
            output,
            _n_size,
            _m_size,
            k_size,
        ),
        GgmlType::Q4_0 | GgmlType::Q5_0 if use_specialized => q_n_sgemv(
            op,
            generic_kernel,
            workgroup_size,
            input_a,
            input_b,
            output,
            _n_size,
            _m_size,
            k_size,
        ),
        GgmlType::Q8_0 if use_specialized => q_8_0_sgemv(
            op,
            generic_kernel,
            workgroup_size,
            input_a,
            input_b,
            output,
            _n_size,
            _m_size,
            k_size,
        ),
        _ => general_sgemv(
            op,
            generic_kernel,
            workgroup_size,
            input_a,
            input_b,
            output,
            _n_size,
            _m_size,
            k_size,
            graph,
        ),
    }
}

pub(crate) fn dispatch_size(matrix: &QMatrix, n: u32, m: u32, batch_size: u32) -> [u32; 3] {
    // Use Y dimension to handle M (each workgroup handles one M value)
    // and X dimension for N (output dimension)
    // Only use specialized dispatch sizes if we can use specialized SGEMV
    if can_use_specialized_sgemv(&matrix.device) {
        if matrix.datatype == GgmlType::Q6K {
            return [n.div_ceil(Q6K_SGEMV_CHUNK_SIZE * 2), m, batch_size];
        }
        if matrix.datatype == GgmlType::Q4K {
            return [n.div_ceil(Q4K_SGEMV_CHUNK_SIZE * 2), m, batch_size];
        }
        if matches!(matrix.datatype, GgmlType::Q4_0 | GgmlType::Q5_0) {
            return [n.div_ceil(Q_N_SGEMV_CHUNK_SIZE * 2), m, batch_size];
        }
        if matches!(matrix.datatype, GgmlType::Q8_0) {
            return [n.div_ceil(Q_8_0_SGEMV_CHUNK_SIZE * 2), m, batch_size];
        }
    }
    [n.div_ceil(SGEMV_CHUNK_SIZE), m, batch_size]
}

pub(crate) fn workgroup_shape_constraints(
    _: &QMatrix,
    device: &Device,
) -> crate::mir::workgroup_shape::WorkgroupShapeConstraints {
    let mut constraints = crate::mir::workgroup_shape::WorkgroupShapeConstraints::default();
    if device.subgroups_supported() {
        constraints.add_constraint(
            0,
            crate::mir::workgroup_shape::Constraint::more_than_or_equals(
                device.min_subgroup_size(),
            ),
        );
        constraints.add_constraint(
            0,
            crate::mir::workgroup_shape::Constraint::less_than_or_equals(
                device.max_subgroup_size(),
            ),
        );
    }
    constraints.add_constraint(1, crate::mir::workgroup_shape::Constraint::Equals(1));
    constraints.add_constraint(2, crate::mir::workgroup_shape::Constraint::Equals(1));
    constraints
}
